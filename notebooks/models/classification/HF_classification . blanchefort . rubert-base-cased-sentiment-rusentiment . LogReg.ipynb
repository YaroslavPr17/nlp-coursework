{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7331aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pristalovya/Документы/nlp-coursework\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f29d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets_ import DatasetLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from nltk import WhitespaceTokenizer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,                       \n",
    "    AutoModelForSequenceClassification,                       \n",
    "    BertForSequenceClassification,                       \n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b713c7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pristalovya/Документы/nlp-coursework/data/reviews_Review_Label/reviews_Review_Label.csv\n",
      "1    48477\n",
      "0     6869\n",
      "Name: label, dtype: int64\n",
      "0    48477\n",
      "1    48477\n",
      "Name: label, dtype: int64\n",
      "(96954, 2) (23721, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6985/3168397547.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train.label[train['label'] == 2] = 1\n",
      "/tmp/ipykernel_6985/3168397547.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test.label[test['label'] == 2] = 1\n"
     ]
    }
   ],
   "source": [
    "train, test = DatasetLoader.load_reviews_Review_Label_dataset(train_test_split=True,\n",
    "                                                              classnames_to_int=True,\n",
    "                                                              remove_neutral_class=True,\n",
    "                                                              show_path=True,)\n",
    "train.label[train['label'] == 2] = 1\n",
    "test.label[test['label'] == 2] = 1\n",
    "\n",
    "print(train.label.value_counts())\n",
    "train = pd.concat([train, resample(train[train.label == 0], n_samples=41608, random_state=42)])\n",
    "print(train.label.value_counts())\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef51e093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25749</th>\n",
       "      <td>Большое количество фильмов советского кинемато...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44489</th>\n",
       "      <td>Тяжело ответить на вопрос, что же такое Догвил...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53162</th>\n",
       "      <td>В наше время такие героини, как скажем наприме...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25843</th>\n",
       "      <td>В 2001 году нам довелось познакомиться с новой...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44609</th>\n",
       "      <td>«Это фильм?», «У них не хватило денег на декор...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14104</th>\n",
       "      <td>- Через столько лет?\\r\\n- Всегда\\r\\n\\r\\nБезусл...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22232</th>\n",
       "      <td>После просмотра трейлера, я был под большим вп...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73314</th>\n",
       "      <td>Многие не верят, но я легко подключаюсь к прои...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47848</th>\n",
       "      <td>Как часто нам нужна поддержка? Да, пожалуй, оч...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10215</th>\n",
       "      <td>15 апреля 2012 исполнилось ровно 100 лет с тог...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23721 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "25749  Большое количество фильмов советского кинемато...      1\n",
       "44489  Тяжело ответить на вопрос, что же такое Догвил...      1\n",
       "53162  В наше время такие героини, как скажем наприме...      0\n",
       "25843  В 2001 году нам довелось познакомиться с новой...      1\n",
       "44609  «Это фильм?», «У них не хватило денег на декор...      1\n",
       "...                                                  ...    ...\n",
       "14104  - Через столько лет?\\r\\n- Всегда\\r\\n\\r\\nБезусл...      1\n",
       "22232  После просмотра трейлера, я был под большим вп...      1\n",
       "73314  Многие не верят, но я легко подключаюсь к прои...      1\n",
       "47848  Как часто нам нужна поддержка? Да, пожалуй, оч...      1\n",
       "10215  15 апреля 2012 исполнилось ровно 100 лет с тог...      1\n",
       "\n",
       "[23721 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "11b4b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_model_input_length=512):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_model_input_length = max_model_input_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        review_tokenized = self.tokenizer(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_model_input_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = review_tokenized['input_ids'].flatten()\n",
    "        attn_mask = review_tokenized['attention_mask'].flatten()\n",
    "        \n",
    "        return {\n",
    "            'review': review,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attn_mask,\n",
    "            'label': label,\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "31bf59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLogRegClassifier:\n",
    "    def __init__(self, checkpoint, n_classes=2):\n",
    "        \n",
    "        self.model = BertForSequenceClassification.from_pretrained(checkpoint)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        \n",
    "        self.max_len = 512\n",
    "        self.out_features = self.model.bert.pooler.dense.out_features\n",
    "        self.model.dropout = torch.nn.Sequential()\n",
    "        self.model.classifier = torch.nn.Sequential()\n",
    "        \n",
    "        self.classifier = LogisticRegression(max_iter=1000, n_jobs=-1, random_state=42)        \n",
    "                \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)    \n",
    "        \n",
    "\n",
    "    def fit(self, train_dataloader: torch.utils.data.DataLoader):\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.all_input_embeddings = np.array([])\n",
    "        \n",
    "        t = tqdm(train_dataloader, file=sys.stdout, ncols=100)\n",
    "\n",
    "        for data in t:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device).to(float)\n",
    "                labels = data['label'].to(self.device)\n",
    "\n",
    "\n",
    "                embeddings = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                ).logits\n",
    "\n",
    "                self.all_input_embeddings = np.append(self.all_input_embeddings, embeddings.cpu().numpy())\n",
    "    \n",
    "        self.all_input_embeddings = self.all_input_embeddings.reshape(-1, self.out_features)\n",
    "    \n",
    "        self.classifier.fit(self.all_input_embeddings, train_dataloader.dataset.labels)            \n",
    "\n",
    "            \n",
    "    def predict(self, test_input):\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.all_output_embeddings = np.array([])\n",
    "        \n",
    "        t = tqdm(test_input, file=sys.stdout, ncols=100)\n",
    "\n",
    "        for data in t:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids = data['input_ids'].to(self.device)\n",
    "                attention_mask = data['attention_mask'].to(self.device).to(float)\n",
    "                labels = data['label'].to(self.device)\n",
    "\n",
    "\n",
    "                embeddings = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                ).logits\n",
    "\n",
    "                self.all_output_embeddings = np.append(self.all_output_embeddings, embeddings.cpu().numpy())\n",
    "    \n",
    "        \n",
    "        self.all_output_embeddings = self.all_output_embeddings.reshape(-1, self.out_features)\n",
    "\n",
    "        return self.classifier.predict(self.all_output_embeddings)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "c806cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BertLogRegClassifier('blanchefort/rubert-base-cased-sentiment-rusentiment')\n",
    "\n",
    "train_dataset = ReviewDataset(train.review, train.label, clf.tokenizer)\n",
    "test_dataset = ReviewDataset(test.review, test.label, clf.tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "67cf50f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters: 177853440\n",
      "Trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "for param in clf.model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "print('All parameters:', sum(p.numel() for p in clf.model.parameters()))\n",
    "print('Trainable parameters:', sum(p.numel() for p in clf.model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5f94593f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1515/1515 [1:13:14<00:00,  2.90s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pristalovya/Inter/linux_packages/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e3aebcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 371/371 [17:39<00:00,  2.85s/it]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.52      0.21      2979\n",
      "           1       0.88      0.51      0.65     20742\n",
      "\n",
      "    accuracy                           0.51     23721\n",
      "   macro avg       0.51      0.52      0.43     23721\n",
      "weighted avg       0.79      0.51      0.59     23721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(test_dataloader)\n",
    "print(classification_report(test.label, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
